{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotnine as plt\n",
    "from plotnine import *\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames = pd.read_csv(\"C:/Users/hblin/Downloads/AmesHousing.csv\")\n",
    "X = ames.drop(\"SalePrice\", axis=1)\n",
    "y = ames[\"SalePrice\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 50591.3232703246\n",
      "Mean RMSE: 55806.32634926364\n",
      "r^2: 0.5687214780092709\n",
      "Cross Val r^2: 0.504208752508862\n"
     ]
    }
   ],
   "source": [
    "# model 1\n",
    "# Size and number of rooms\n",
    "ct_dummies = ColumnTransformer(\n",
    "  [(\"standardize\", StandardScaler(), [\"Gr Liv Area\", \"TotRms AbvGrd\"])],\n",
    "  remainder = \"drop\"\n",
    ")\n",
    "\n",
    "lr_pipeline = Pipeline(\n",
    "  [(\"preprocessing\", ct_dummies),\n",
    "  (\"linear_regression\", LinearRegression())]\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "y_pred = lr_pipeline.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "scores = cross_val_score(lr_pipeline, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Calculate the mean RMSE\n",
    "mean_rmse = -scores.mean()  # Take the negative to convert back to positive RMSE\n",
    "print(f\"Mean RMSE: {mean_rmse}\")\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"r^2: {r2}\")\n",
    "\n",
    "cross_val_r2 = cross_val_score(lr_pipeline, X, y, cv=5, scoring='r2').mean()\n",
    "print(f\"Cross Val r^2: {cross_val_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 49047.62094866008\n",
      "Mean RMSE: 54153.20862794976\n",
      "r^2: 0.5946392954747666\n",
      "Cross Val r^2: 0.5331485871994233\n"
     ]
    }
   ],
   "source": [
    "# model 2\n",
    "ct_dummies = ColumnTransformer(\n",
    "  [(\"dummify\", OneHotEncoder(sparse_output = False), [\"Bldg Type\"]),\n",
    "  (\"standardize\", StandardScaler(), [\"Gr Liv Area\", \"TotRms AbvGrd\"])],\n",
    "  remainder = \"drop\"\n",
    ")\n",
    "\n",
    "lr_pipeline = Pipeline(\n",
    "  [(\"preprocessing\", ct_dummies),\n",
    "  (\"linear_regression\", LinearRegression())]\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "y_pred = lr_pipeline.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "scores = cross_val_score(lr_pipeline, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Calculate the mean RMSE\n",
    "mean_rmse = -scores.mean()  # Take the negative to convert back to positive RMSE\n",
    "print(f\"Mean RMSE: {mean_rmse}\")\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"r^2: {r2}\")\n",
    "\n",
    "cross_val_r2 = cross_val_score(lr_pipeline, X, y, cv=5, scoring='r2').mean()\n",
    "print(f\"Cross Val r^2: {cross_val_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 48417.098629975684\n",
      "Mean RMSE: 54751.69903028269\n",
      "r^2: 0.6049943801381477\n",
      "Cross Val r^2: 0.5353239424795158\n"
     ]
    }
   ],
   "source": [
    "# First step: Dummy encoding and standardization\n",
    "ct_dummies = ColumnTransformer(\n",
    "    [\n",
    "        (\"dummify\", OneHotEncoder(drop=\"first\", sparse_output=False), [\"Bldg Type\"]),\n",
    "        (\"standardize\", StandardScaler(), [\"Gr Liv Area\"])\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "# Apply the preprocessing transformer first\n",
    "X_train_transformed = ct_dummies.fit_transform(X_train)\n",
    "X_test_transformed = ct_dummies.transform(X_test)\n",
    "\n",
    "# Now you can access the encoded columns\n",
    "encoded_columns = X_train_transformed.columns\n",
    "\n",
    "# Define the interaction columns for the transformed (encoded) data\n",
    "interaction_columns = [col for col in encoded_columns if \"Bldg Type\" in col]  # List of encoded \"Bldg Type\" columns\n",
    "\n",
    "# Add the continuous feature column name\n",
    "gr_liv_area_col = \"standardize__Gr Liv Area\"  # Verify this column name matches exactly\n",
    "\n",
    "# Define interactions using transformed column names\n",
    "# You can apply PolynomialFeatures on the relevant columns\n",
    "ct_inter = ColumnTransformer(\n",
    "    [\n",
    "        ('interaction', PolynomialFeatures(interaction_only=True, include_bias=False),\n",
    "         [gr_liv_area_col] + interaction_columns)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "# Apply the interactions on top of the first transformation\n",
    "X_train_interactions = ct_inter.fit_transform(X_train_transformed)\n",
    "X_test_interactions = ct_inter.transform(X_test_transformed)\n",
    "\n",
    "# Now, define the final pipeline for fitting\n",
    "lr_pipeline = Pipeline([(\"linear_regression\", LinearRegression())])\n",
    "\n",
    "# Fit the model on transformed data\n",
    "lr_pipeline.fit(X_train_interactions, y_train)\n",
    "y_pred = lr_pipeline.predict(X_test_interactions)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Cross-validation on the transformed data\n",
    "scores = cross_val_score(lr_pipeline, X_train_interactions, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "mean_rmse = -scores.mean()\n",
    "print(f\"Mean RMSE: {mean_rmse}\")\n",
    "\n",
    "# Calculate r-squared for the test set\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"r^2: {r2}\")\n",
    "\n",
    "# Cross-validation r-squared\n",
    "cross_val_r2 = cross_val_score(lr_pipeline, X_train_interactions, y_train, cv=5, scoring='r2').mean()\n",
    "print(f\"Cross Val r^2: {cross_val_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 49092.35050568161\n",
      "Mean RMSE: 55176.96594338035\n",
      "r^2: 0.5938996113856301\n",
      "Cross Val r^2: 0.5106643234404193\n"
     ]
    }
   ],
   "source": [
    "# model 4\n",
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        (\"dummify\", OneHotEncoder(sparse_output=False), [\"Bldg Type\"]),\n",
    "        (\"poly_size\", Pipeline([\n",
    "            (\"standardize\", StandardScaler()), \n",
    "            (\"poly\", PolynomialFeatures(degree=5, include_bias=False))\n",
    "        ]), [\"Gr Liv Area\"]),\n",
    "        (\"poly_rooms\", Pipeline([\n",
    "            (\"standardize\", StandardScaler()), \n",
    "            (\"poly\", PolynomialFeatures(degree=5, include_bias=False))\n",
    "        ]), [\"TotRms AbvGrd\"])\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "lr_pipeline = Pipeline(\n",
    "  [(\"preprocessing\", preprocessing),\n",
    "  (\"linear_regression\", LinearRegression())]\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "#lr_pipeline.named_steps[\"preprocessing\"].get_feature_names_out()\n",
    "\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "y_pred = lr_pipeline.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "scores = cross_val_score(lr_pipeline, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Calculate the mean RMSE\n",
    "mean_rmse = -scores.mean()  # Take the negative to convert back to positive RMSE\n",
    "print(f\"Mean RMSE: {mean_rmse}\")\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"r^2: {r2}\")\n",
    "\n",
    "cross_val_r2 = cross_val_score(lr_pipeline, X, y, cv=5, scoring='r2').mean()\n",
    "print(f\"Cross Val r^2: {cross_val_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to these models, model 3 seems to have performed the best.\n",
    "\n",
    "With the cross validation, the RMSE's also agree that model 3 has performed the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    degrees_gr_liv_area  degrees_totrms_abvgrd     scores\n",
      "0                     1                      1   0.532882\n",
      "1                     1                      2   0.532383\n",
      "2                     1                      3   0.535924\n",
      "3                     1                      4   0.541529\n",
      "4                     1                      5   0.541066\n",
      "..                  ...                    ...        ...\n",
      "95                   10                      6 -16.187618\n",
      "96                   10                      7 -16.187618\n",
      "97                   10                      8 -16.187618\n",
      "98                   10                      9 -16.187618\n",
      "99                   10                     10 -16.187618\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Best parameters: {'preprocessing__poly_gr_liv_area__degree': np.int64(3), 'preprocessing__poly_totrms_abvgrd__degree': np.int64(1)}\n",
      "Best R-squared score: 0.5576406283340998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define ColumnTransformer with separate PolynomialFeatures for each numeric feature\n",
    "ct_poly = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"dummify\", OneHotEncoder(sparse_output=False), [\"Bldg Type\"]),\n",
    "        (\"poly_gr_liv_area\", PolynomialFeatures(include_bias=False), [\"Gr Liv Area\"]),\n",
    "        (\"poly_totrms_abvgrd\", PolynomialFeatures(include_bias=False), [\"TotRms AbvGrd\"])\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Define the pipeline with separate polynomial transformers\n",
    "lr_pipeline_poly = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessing\", ct_poly),\n",
    "        (\"linear_regression\", LinearRegression())\n",
    "    ]\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "# Define a grid with separate degrees for each feature\n",
    "degrees = {\n",
    "    'preprocessing__poly_gr_liv_area__degree': np.arange(1, 11),\n",
    "    'preprocessing__poly_totrms_abvgrd__degree': np.arange(1, 11)\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "gscv = GridSearchCV(lr_pipeline_poly, degrees, cv=5, scoring='r2')\n",
    "gscv_fitted = gscv.fit(X, y)\n",
    "\n",
    "# View the results\n",
    "print(pd.DataFrame({\n",
    "    \"degrees_gr_liv_area\": gscv.cv_results_['param_preprocessing__poly_gr_liv_area__degree'],\n",
    "    \"degrees_totrms_abvgrd\": gscv.cv_results_['param_preprocessing__poly_totrms_abvgrd__degree'],\n",
    "    \"scores\": gscv.cv_results_['mean_test_score']\n",
    "}))\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", gscv.best_params_)\n",
    "print(\"Best R-squared score:\", gscv.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the model with size of degree 3 and rooms  of degree 1 perfomred the best. The downsides of this are that this could possibly take a long time in terms of computing power, and if we needed to do this quickly that would not be optimal. We could also limit the polynomial to a number of degrees that makes sense so we do not overfit the model or add too much complexity unnecessarily. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
