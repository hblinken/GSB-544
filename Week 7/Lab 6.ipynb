{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Lab 6'\n",
    "author: \"Hunter Blinkenberg\"\n",
    "embed-resources: true\n",
    "format: \n",
    "    html:\n",
    "        toc: true\n",
    "        code-fold: true\n",
    "        theme: cosmo\n",
    "        echo: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethics Statement and References: This lab uses predictive modeling to design a model that predicts a baseball player's salary. Generative AI was used to help out with some of the functions that were more difficult to figure out such as how to get the ceofficients names to display along with the actual value, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from plotnine import ggplot, aes, geom_bar, theme_minimal, labs, facet_wrap, geom_point\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = pd.read_csv(\"C:/Users/hblin/OneDrive - Cal Poly/GSB 544/Week 7/Hitters.csv\")\n",
    "\n",
    "hits = hits.dropna()\n",
    "\n",
    "# hits[\"Salary\"] = hits[\"Salary\"] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hits.drop(\"Salary\", axis= 1)\n",
    "y = hits[\"Salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression_with_cv(X, y, regression_type):\n",
    "    \"\"\"\n",
    "    Function to run a regression (OLS, Ridge, Lasso, ElasticNet) with cross-validation\n",
    "    and return the best hyperparameter (alpha) and best cross-validated MSE score.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Features DataFrame\n",
    "    y -- Target variable\n",
    "    regression_type -- Type of regression to run: 'ols', 'ridge', 'lasso', 'elasticnet'\n",
    "\n",
    "    Returns:\n",
    "    best_alpha -- Best alpha parameter found via GridSearchCV (or None for OLS)\n",
    "    best_cv_mse -- Best cross-validated MSE score\n",
    "    \"\"\"\n",
    "    # Define ColumnTransformer for preprocessing\n",
    "    ct = ColumnTransformer(\n",
    "        [\n",
    "            (\"dummify\", OneHotEncoder(sparse_output=False, handle_unknown='ignore'), make_column_selector(dtype_include=object)),\n",
    "            (\"standardize\", StandardScaler(), make_column_selector(dtype_include=np.number))\n",
    "        ],\n",
    "        remainder=\"passthrough\"\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    # Initialize the regression models\n",
    "    if regression_type == 'ols':\n",
    "        model = LinearRegression()\n",
    "        pipeline = Pipeline([(\"preprocessing\", ct), (\"regression\", model)])\n",
    "        # Cross-validation MSE score\n",
    "        best_cv_mse = cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_squared_error').mean() * -1\n",
    "        pipeline.fit(X, y)  # Fit to get the coefficients\n",
    "\n",
    "        # Extract feature names after one-hot encoding for categorical features\n",
    "        categorical_feature_names = ct.transformers_[0][1].get_feature_names_out(input_features=X.select_dtypes(include=[object]).columns)\n",
    "\n",
    "        # Get numerical feature names\n",
    "        numerical_feature_names = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "        # Combine all feature names (categorical and numeric)\n",
    "        all_feature_names = list(categorical_feature_names) + list(numerical_feature_names)\n",
    "\n",
    "        # Extract coefficients for the fitted model\n",
    "        coefficients = pipeline.named_steps['regression'].coef_\n",
    "\n",
    "        # Display coefficients with feature names\n",
    "        print(\"Feature names and coefficients for OLS (Linear Regression):\")\n",
    "        for feature, coef in zip(all_feature_names, coefficients):\n",
    "            print(f\"{feature}: {coef}\")\n",
    "        \n",
    "        return None, best_cv_mse  # No best alpha for OLS\n",
    "\n",
    "    elif regression_type == 'ridge':\n",
    "        model = Ridge()\n",
    "        pipeline = Pipeline([(\"preprocessing\", ct), (\"ridge\", model)])\n",
    "\n",
    "        # Hyperparameter grid for Ridge\n",
    "        param_grid = {\"ridge__alpha\": [0.001, 0.01, 0.1, 1, 10]}\n",
    "    \n",
    "    elif regression_type == 'lasso':\n",
    "        model = Lasso()\n",
    "        pipeline = Pipeline([(\"preprocessing\", ct), (\"lasso\", model)])\n",
    "\n",
    "        # Hyperparameter grid for Lasso\n",
    "        param_grid = {\"lasso__alpha\": [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "    elif regression_type == 'elasticnet':\n",
    "        model = ElasticNet()\n",
    "        pipeline = Pipeline([(\"preprocessing\", ct), (\"elasticnet\", model)])\n",
    "\n",
    "        # Hyperparameter grid for ElasticNet\n",
    "        param_grid = {\"elasticnet__alpha\": [0.001, 0.01, 0.1, 1, 10],\n",
    "                      \"elasticnet__l1_ratio\": [0.1, 0.5, 0.9]}  # L1 ratio for ElasticNet\n",
    "\n",
    "    # GridSearchCV for finding the best alpha\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Best hyperparameter and cross-validated MSE score\n",
    "    best_alpha = grid_search.best_params_\n",
    "    best_cv_mse = -grid_search.best_score_  # We negate the score to get the positive MSE\n",
    "    \n",
    "    # Display the results\n",
    "    print(f\"Best {regression_type} Alpha: {best_alpha}\")\n",
    "    print(f\"Best Cross-Validated MSE: {best_cv_mse}\")\n",
    "\n",
    "    # Extract the coefficients and feature names for the best model\n",
    "    best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "    # Get the preprocessor and model from the pipeline\n",
    "    preprocessor = best_pipeline.named_steps['preprocessing']\n",
    "    \n",
    "    # Extract feature names for categorical columns\n",
    "    cat_features = preprocessor.named_transformers_['dummify'].get_feature_names_out(input_features=X.select_dtypes(include=[object]).columns)\n",
    "    \n",
    "    # Extract feature names for numerical columns\n",
    "    num_features = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    # Combine both categorical and numerical feature names\n",
    "    all_feature_names = list(cat_features) + list(num_features)\n",
    "    \n",
    "    # Extract coefficients for the selected model\n",
    "    coefficients = best_pipeline.named_steps[regression_type].coef_\n",
    "\n",
    "    # Combine feature names and coefficients\n",
    "    print(f\"Feature names and coefficients for {regression_type}:\")\n",
    "    for feature, coef in zip(all_feature_names, coefficients):\n",
    "        print(f\"{feature}: {coef}\")\n",
    "\n",
    "    return best_alpha, best_cv_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Different Model Specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  A. Regression without regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names and coefficients for OLS (Linear Regression):\n",
      "League_A: -31.299711517594083\n",
      "League_N: 31.29971151759421\n",
      "Division_E: 58.424622818437655\n",
      "Division_W: -58.424622818437655\n",
      "NewLeague_A: 12.381162554200026\n",
      "NewLeague_N: -12.381162554200296\n",
      "AtBat: -291.09455569715004\n",
      "Hits: 337.8304794814834\n",
      "HmRun: 37.853836764343086\n",
      "Runs: -60.572478605510774\n",
      "RBI: -26.99498378992079\n",
      "Walks: 135.0738969513123\n",
      "Years: -16.69335887510292\n",
      "CAtBat: -391.038654663538\n",
      "CHits: 86.68761663772092\n",
      "CHmRun: -14.18172332300863\n",
      "CRuns: 480.7471347707918\n",
      "CRBI: 260.68988580523114\n",
      "CWalks: -213.89225864291225\n",
      "PutOuts: 78.76129639492689\n",
      "Assists: 53.73248973474442\n",
      "Errors: -22.160862174225585\n",
      "Best alpha (Ridge): None\n",
      "Best Cross-Validated MSE: 121136.31031816886\n"
     ]
    }
   ],
   "source": [
    "best_alpha, best_cv_mse = run_regression_with_cv(X, y, regression_type='ols')\n",
    "print(f\"Best alpha (Ridge): {best_alpha}\")\n",
    "print(f\"Best Cross-Validated MSE: {best_cv_mse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most imporant coefficients are Hits, CRuns, CAtBat, CRBI, and AtBat. These suggest that hitting capabilities largely affect salary. CRuns is the biggest predictor coefficient with a predicted 480k salary increase for every career run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ridge Alpha: {'ridge__alpha': 1}\n",
      "Best Cross-Validated MSE: 119144.43267691605\n",
      "Feature names and coefficients for ridge:\n",
      "League_A: -30.438855310625804\n",
      "League_N: 30.438855310628888\n",
      "Division_E: 60.01559492710364\n",
      "Division_W: -60.015594927103336\n",
      "NewLeague_A: 13.111281545686666\n",
      "NewLeague_N: -13.111281545685904\n",
      "AtBat: -270.68644069623673\n",
      "Hits: 296.64505003257005\n",
      "HmRun: 18.100591581199478\n",
      "Runs: -29.339406130418972\n",
      "RBI: -9.113294533329986\n",
      "Walks: 124.40717273179192\n",
      "Years: -38.66774782234878\n",
      "CAtBat: -225.40654798156194\n",
      "CHits: 126.65960654936816\n",
      "CHmRun: 39.07092364167405\n",
      "CRuns: 320.4121689078557\n",
      "CRBI: 160.38678418275893\n",
      "CWalks: -184.42361059971174\n",
      "PutOuts: 78.62365619201013\n",
      "Assists: 47.462597110689764\n",
      "Errors: -23.724190306614684\n",
      "Best alpha (Ridge): {'ridge__alpha': 1}\n",
      "Best Cross-Validated MSE: 119144.43267691605\n"
     ]
    }
   ],
   "source": [
    "best_alpha, best_cv_mse = run_regression_with_cv(X, y, regression_type='ridge')\n",
    "print(f\"Best alpha (Ridge): {best_alpha}\")\n",
    "print(f\"Best Cross-Validated MSE: {best_cv_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ridge model, CRuns is still the biggest predictor coefficient with a predicted 320k salary increase for every career run. The same variables of hitting capabilities seem to remain most important  over the career. In this model, AtBat is the next biggest predicter coefficient with a predicted -270k salary decrease with every 1 at bat. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.004e+07, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.012e+07, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.096e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.537e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.594e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.059e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.591e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.471e+05, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.066e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.923e+04, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+05, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.158e+05, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.929e+04, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.282e+05, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.815e+03, tolerance: 4.281e+03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lasso Alpha: {'lasso__alpha': 1}\n",
      "Best Cross-Validated MSE: 119761.58740741308\n",
      "Feature names and coefficients for lasso:\n",
      "League_A: -35.826072111628456\n",
      "League_N: 9.974641468119858e-14\n",
      "Division_E: 114.41295113388867\n",
      "Division_W: -2.0789294977294632e-11\n",
      "NewLeague_A: 0.0\n",
      "NewLeague_N: -0.0\n",
      "AtBat: -282.37095717330493\n",
      "Hits: 304.35950858415094\n",
      "HmRun: 11.127022029363774\n",
      "Runs: -24.96650711139829\n",
      "RBI: -0.0\n",
      "Walks: 120.69527502165931\n",
      "Years: -34.94814807264341\n",
      "CAtBat: -162.6397937686367\n",
      "CHits: 0.0\n",
      "CHmRun: 14.225993157039428\n",
      "CRuns: 375.5655192079644\n",
      "CRBI: 192.61089166933445\n",
      "CWalks: -189.6446419428977\n",
      "PutOuts: 78.76036575456914\n",
      "Assists: 41.99667950395398\n",
      "Errors: -18.479378396747396\n",
      "Best alpha (Ridge): {'lasso__alpha': 1}\n",
      "Best Cross-Validated MSE: 119761.58740741308\n"
     ]
    }
   ],
   "source": [
    "best_alpha, best_cv_mse = run_regression_with_cv(X, y, regression_type='lasso')\n",
    "print(f\"Best alpha (Ridge): {best_alpha}\")\n",
    "print(f\"Best Cross-Validated MSE: {best_cv_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the last models, CRuns is the biggest predictor, with CRuns, Hits, AtBat, and CWalks are weighted the heaviest. Some coefficients have been forced to 0 as a result of the lasso regression though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Elastic Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.033e+07, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.137e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.030e+07, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.359e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.710e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.012e+07, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.995e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+07, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.060e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.579e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+07, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.982e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.013e+07, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.488e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.543e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.597e+05, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.940e+05, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.672e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.925e+05, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.683e+05, tolerance: 4.558e+03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best elasticnet Alpha: {'elasticnet__alpha': 0.1, 'elasticnet__l1_ratio': 0.9}\n",
      "Best Cross-Validated MSE: 118969.49340131154\n",
      "Feature names and coefficients for elasticnet:\n",
      "League_A: -27.879684823579147\n",
      "League_N: 27.87998426682858\n",
      "Division_E: 60.67059232883931\n",
      "Division_W: -60.67174944853942\n",
      "NewLeague_A: 11.19158041766919\n",
      "NewLeague_N: -11.19347297383407\n",
      "AtBat: -231.50781166705525\n",
      "Hits: 247.55614590053753\n",
      "HmRun: 4.470921754490769\n",
      "Runs: -5.041667988969194\n",
      "RBI: 2.384347282527404\n",
      "Walks: 110.80823487974342\n",
      "Years: -49.54157197831513\n",
      "CAtBat: -115.29371139914225\n",
      "CHits: 120.01542461996134\n",
      "CHmRun: 55.91505651718336\n",
      "CRuns: 223.03749016380291\n",
      "CRBI: 121.69342970573753\n",
      "CWalks: -154.66882940112455\n",
      "PutOuts: 77.91853947991538\n",
      "Assists: 40.7366413934229\n",
      "Errors: -24.388980915134283\n",
      "Best alpha (Ridge): {'elasticnet__alpha': 0.1, 'elasticnet__l1_ratio': 0.9}\n",
      "Best Cross-Validated MSE: 118969.49340131154\n"
     ]
    }
   ],
   "source": [
    "best_alpha, best_cv_mse = run_regression_with_cv(X, y, regression_type='elasticnet')\n",
    "print(f\"Best alpha (Ridge): {best_alpha}\")\n",
    "print(f\"Best Cross-Validated MSE: {best_cv_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to other models, Hits is the biggest predictor, with CRuns, Hits, AtBat, and CWalks being weighted some of the heaviest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most important numeric: CRuns \n",
    "\n",
    "Five most important numeric: CRuns, Hits, AtBat, Walks, and CRBI\n",
    "\n",
    "Most important Categorical: Division "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xone = hits['CRuns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_regression_with_one(Xone, y, regression_type):\n",
    "    \"\"\"\n",
    "    Function to run a regression (OLS, Ridge, Lasso, ElasticNet) with cross-validation\n",
    "    and return the best hyperparameter (alpha) and best cross-validated MSE score.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Features DataFrame\n",
    "    y -- Target variable\n",
    "    regression_type -- Type of regression to run: 'ols', 'ridge', 'lasso', 'elasticnet'\n",
    "\n",
    "    Returns:\n",
    "    best_alpha -- Best alpha parameter found via GridSearchCV (or None for OLS)\n",
    "    best_cv_mse -- Best cross-validated MSE score\n",
    "    \"\"\"\n",
    "    # Convert all non-numeric columns to numeric (or handle non-numeric values appropriately)\n",
    "    #print(\"Starting preprocessing...\")\n",
    "    Xone = Xone.apply(pd.to_numeric, errors='coerce')  # Coerce non-numeric to NaN\n",
    "    Xone.fillna(0, inplace=True)  # Optionally fill NaN values with 0 (or another strategy)\n",
    "\n",
    "    print(\"Preprocessing completed.\")\n",
    "\n",
    "    # Define ColumnTransformer for preprocessing (standardizing numerical, encoding categorical)\n",
    "    numerical_cols = Xone.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = Xone.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    ct = ColumnTransformer(\n",
    "        [\n",
    "            (\"standardize\", StandardScaler(), numerical_cols),  # Standard scaling for numerical features\n",
    "            (\"encode\", OneHotEncoder(), categorical_cols),  # One-hot encoding for categorical features\n",
    "        ],\n",
    "        remainder=\"passthrough\"\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    # Initialize the regression models\n",
    "    pipeline = None  # Initialize the pipeline variable to avoid UnboundLocalError\n",
    "\n",
    "    if regression_type == 'ols':\n",
    "        print(\"OLS Regression selected.\")\n",
    "        model = LinearRegression()\n",
    "        pipeline = Pipeline([(\"preprocessing\", ct), (\"regression\", model)])\n",
    "        \n",
    "        # Cross-validation MSE score\n",
    "        best_cv_mse = cross_val_score(pipeline, Xone, y, cv=5, scoring='neg_mean_squared_error').mean() * -1\n",
    "        pipeline.fit(Xone, y)  # Fit to get the coefficients\n",
    "\n",
    "        # Extract feature names\n",
    "        all_feature_names = list(numerical_cols) + list(categorical_cols)\n",
    "\n",
    "        # Extract coefficients for the fitted model\n",
    "        coefficients = pipeline.named_steps['regression'].coef_\n",
    "\n",
    "        # Display coefficients with feature names\n",
    "        print(\"Feature names and coefficients for OLS (Linear Regression):\")\n",
    "        for feature, coef in zip(all_feature_names, coefficients):\n",
    "            print(f\"{feature}: {coef}\")\n",
    "\n",
    "        return None, best_cv_mse  # No best alpha for OLS\n",
    "\n",
    "    # For Ridge, Lasso, and ElasticNet\n",
    "    elif regression_type in ['ridge', 'lasso', 'elasticnet']:\n",
    "        print(f\"{regression_type} selected.\")\n",
    "        # Set model and hyperparameters based on the regression type\n",
    "        if regression_type == 'ridge':\n",
    "            model = Ridge()\n",
    "            param_grid = {\"ridge__alpha\": [0.001, 0.01, 0.1, 1, 10]}\n",
    "        elif regression_type == 'lasso':\n",
    "            model = Lasso()\n",
    "            param_grid = {\"lasso__alpha\": [0.001, 0.01, 0.1, 1, 10]}\n",
    "        elif regression_type == 'elasticnet':\n",
    "            model = ElasticNet()\n",
    "            param_grid = {\"elasticnet__alpha\": [0.001, 0.01, 0.1, 1, 10],\n",
    "                          \"elasticnet__l1_ratio\": [0.1, 0.5, 0.9]}  # L1 ratio for ElasticNet\n",
    "\n",
    "        # Create pipeline and perform grid search\n",
    "        pipeline = Pipeline([(\"preprocessing\", ct), (regression_type, model)])  # Create pipeline here\n",
    "        print(\"Performing GridSearchCV...\")\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(Xone, y)\n",
    "\n",
    "        # Best hyperparameter and cross-validated MSE score\n",
    "        best_alpha = grid_search.best_params_\n",
    "        best_cv_mse = -grid_search.best_score_  # Negating to return positive MSE\n",
    "        \n",
    "        # Display the results\n",
    "        print(f\"Best {regression_type} Alpha: {best_alpha}\")\n",
    "        print(f\"Best Cross-Validated MSE: {best_cv_mse}\")\n",
    "\n",
    "        # Extract the coefficients and feature names for the best model\n",
    "        best_pipeline = grid_search.best_estimator_\n",
    "        coefficients = best_pipeline.named_steps[regression_type].coef_\n",
    "\n",
    "        # Combine feature names (numerical + categorical)\n",
    "        all_feature_names = list(numerical_cols) + list(categorical_cols)\n",
    "\n",
    "        # Display coefficients for the selected model\n",
    "        print(f\"Feature names and coefficients for {regression_type}:\")\n",
    "        for feature, coef in zip(all_feature_names, coefficients):\n",
    "            print(f\"{feature}: {coef}\")\n",
    "\n",
    "        return best_alpha, best_cv_mse\n",
    "\n",
    "    else:\n",
    "        print(f\"Invalid regression type: {regression_type}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "OLS Regression selected.\n",
      "Feature names and coefficients for OLS (Linear Regression):\n",
      "CRuns: 253.35139209799252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, np.float64(143812.93591629734))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regression_with_one(Xone, y, regression_type= \"ols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "ridge selected.\n",
      "Performing GridSearchCV...\n",
      "Best ridge Alpha: {'ridge__alpha': 10}\n",
      "Best Cross-Validated MSE: 143658.5173685888\n",
      "Feature names and coefficients for ridge:\n",
      "CRuns: 244.07112132517216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'ridge__alpha': 10}, np.float64(143658.5173685888))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regression_with_one(Xone, y, regression_type= \"ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "lasso selected.\n",
      "Performing GridSearchCV...\n",
      "Best lasso Alpha: {'lasso__alpha': 10}\n",
      "Best Cross-Validated MSE: 143793.4491585397\n",
      "Feature names and coefficients for lasso:\n",
      "CRuns: 243.3513920979925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'lasso__alpha': 10}, np.float64(143793.4491585397))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regression_with_one(Xone, y, regression_type= \"lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "elasticnet selected.\n",
      "Performing GridSearchCV...\n",
      "Best elasticnet Alpha: {'elasticnet__alpha': 0.1, 'elasticnet__l1_ratio': 0.5}\n",
      "Best Cross-Validated MSE: 143655.07604766646\n",
      "Feature names and coefficients for elasticnet:\n",
      "CRuns: 241.23942104570716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'elasticnet__alpha': 0.1, 'elasticnet__l1_ratio': 0.5},\n",
       " np.float64(143655.07604766646))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regression_with_one(Xone, y, regression_type= \"elasticnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Five Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfive = hits[['CRuns', 'Hits', 'AtBat', 'Walks', 'CRBI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression_with_five(Xfive, y, regression_type):\n",
    "    \"\"\"\n",
    "    Function to run a regression (OLS, Ridge, Lasso, ElasticNet) with cross-validation\n",
    "    and return the best hyperparameter (alpha) and best cross-validated MSE score.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Features DataFrame\n",
    "    y -- Target variable\n",
    "    regression_type -- Type of regression to run: 'ols', 'ridge', 'lasso', 'elasticnet'\n",
    "\n",
    "    Returns:\n",
    "    best_alpha -- Best alpha parameter found via GridSearchCV (or None for OLS)\n",
    "    best_cv_mse -- Best cross-validated MSE score\n",
    "    \"\"\"\n",
    "    # Convert all non-numeric columns to numeric (or handle non-numeric values appropriately)\n",
    "    #print(\"Starting preprocessing...\")\n",
    "    Xfive = Xfive.apply(pd.to_numeric, errors='coerce')  # Coerce non-numeric to NaN\n",
    "    Xfive.fillna(0, inplace=True)  # Optionally fill NaN values with 0 (or another strategy)\n",
    "\n",
    "    print(\"Preprocessing completed.\")\n",
    "\n",
    "    # Define ColumnTransformer for preprocessing (standardizing numerical, encoding categorical)\n",
    "    numerical_cols = Xfive.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = Xfive.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    ct = ColumnTransformer(\n",
    "        [\n",
    "            (\"standardize\", StandardScaler(), numerical_cols),  # Standard scaling for numerical features\n",
    "            (\"encode\", OneHotEncoder(), categorical_cols),  # One-hot encoding for categorical features\n",
    "        ],\n",
    "        remainder=\"passthrough\"\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    # Initialize the regression models\n",
    "    pipeline = None  # Initialize the pipeline variable to avoid UnboundLocalError\n",
    "\n",
    "    if regression_type == 'ols':\n",
    "        print(\"OLS Regression selected.\")\n",
    "        model = LinearRegression()\n",
    "        pipeline = Pipeline([(\"preprocessing\", ct), (\"regression\", model)])\n",
    "        \n",
    "        # Cross-validation MSE score\n",
    "        best_cv_mse = cross_val_score(pipeline, Xfive, y, cv=5, scoring='neg_mean_squared_error').mean() * -1\n",
    "        pipeline.fit(Xfive, y)  # Fit to get the coefficients\n",
    "\n",
    "        # Extract feature names\n",
    "        all_feature_names = list(numerical_cols) + list(categorical_cols)\n",
    "\n",
    "        # Extract coefficients for the fitted model\n",
    "        coefficients = pipeline.named_steps['regression'].coef_\n",
    "\n",
    "        # Display coefficients with feature names\n",
    "        print(\"Feature names and coefficients for OLS (Linear Regression):\")\n",
    "        for feature, coef in zip(all_feature_names, coefficients):\n",
    "            print(f\"{feature}: {coef}\")\n",
    "\n",
    "        return None, best_cv_mse  # No best alpha for OLS\n",
    "\n",
    "    # For Ridge, Lasso, and ElasticNet\n",
    "    elif regression_type in ['ridge', 'lasso', 'elasticnet']:\n",
    "        print(f\"{regression_type} selected.\")\n",
    "        # Set model and hyperparameters based on the regression type\n",
    "        if regression_type == 'ridge':\n",
    "            model = Ridge()\n",
    "            param_grid = {\"ridge__alpha\": [0.001, 0.01, 0.1, 1, 10]}\n",
    "        elif regression_type == 'lasso':\n",
    "            model = Lasso()\n",
    "            param_grid = {\"lasso__alpha\": [0.001, 0.01, 0.1, 1, 10]}\n",
    "        elif regression_type == 'elasticnet':\n",
    "            model = ElasticNet()\n",
    "            param_grid = {\"elasticnet__alpha\": [0.001, 0.01, 0.1, 1, 10],\n",
    "                          \"elasticnet__l1_ratio\": [0.1, 0.5, 0.9]}  # L1 ratio for ElasticNet\n",
    "\n",
    "        # Create pipeline and perform grid search\n",
    "        pipeline = Pipeline([(\"preprocessing\", ct), (regression_type, model)])  # Create pipeline here\n",
    "        print(\"Performing GridSearchCV...\")\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(Xfive, y)\n",
    "\n",
    "        # Best hyperparameter and cross-validated MSE score\n",
    "        best_alpha = grid_search.best_params_\n",
    "        best_cv_mse = -grid_search.best_score_  # Negating to return positive MSE\n",
    "        \n",
    "        # Display the results\n",
    "        print(f\"Best {regression_type} Alpha: {best_alpha}\")\n",
    "        print(f\"Best Cross-Validated MSE: {best_cv_mse}\")\n",
    "\n",
    "        # Extract the coefficients and feature names for the best model\n",
    "        best_pipeline = grid_search.best_estimator_\n",
    "        coefficients = best_pipeline.named_steps[regression_type].coef_\n",
    "\n",
    "        # Combine feature names (numerical + categorical)\n",
    "        all_feature_names = list(numerical_cols) + list(categorical_cols)\n",
    "\n",
    "        # Display coefficients for the selected model\n",
    "        print(f\"Feature names and coefficients for {regression_type}:\")\n",
    "        for feature, coef in zip(all_feature_names, coefficients):\n",
    "            print(f\"{feature}: {coef}\")\n",
    "\n",
    "        return best_alpha, best_cv_mse\n",
    "\n",
    "    else:\n",
    "        print(f\"Invalid regression type: {regression_type}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "OLS Regression selected.\n",
      "Feature names and coefficients for OLS (Linear Regression):\n",
      "CRuns: 45.13222426606409\n",
      "Hits: 368.80583284198326\n",
      "AtBat: -283.49581188398906\n",
      "Walks: 93.59915164465491\n",
      "CRBI: 165.18729094655532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, np.float64(119906.79075868055))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regression_with_five(Xfive, y, regression_type= \"ols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "ridge selected.\n",
      "Performing GridSearchCV...\n",
      "Best ridge Alpha: {'ridge__alpha': 1}\n",
      "Best Cross-Validated MSE: 119386.6696482726\n",
      "Feature names and coefficients for ridge:\n",
      "CRuns: 50.362476868900835\n",
      "Hits: 336.9126865434994\n",
      "AtBat: -250.39048519333912\n",
      "Walks: 91.17324634130557\n",
      "CRBI: 160.05870566725412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'ridge__alpha': 1}, np.float64(119386.6696482726))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regression_with_five(Xfive, y, regression_type= \"ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "lasso selected.\n",
      "Performing GridSearchCV...\n",
      "Best lasso Alpha: {'lasso__alpha': 1}\n",
      "Best Cross-Validated MSE: 119509.30500015007\n",
      "Feature names and coefficients for lasso:\n",
      "CRuns: 46.1892639320874\n",
      "Hits: 340.1445375678188\n",
      "AtBat: -252.76090304072306\n",
      "Walks: 90.35855660176503\n",
      "CRBI: 163.68433349414158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'lasso__alpha': 1}, np.float64(119509.30500015007))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regression_with_five(Xfive, y, regression_type= \"lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "elasticnet selected.\n",
      "Performing GridSearchCV...\n",
      "Best elasticnet Alpha: {'elasticnet__alpha': 0.1, 'elasticnet__l1_ratio': 0.9}\n",
      "Best Cross-Validated MSE: 119175.85350719497\n",
      "Feature names and coefficients for elasticnet:\n",
      "CRuns: 57.13906103866921\n",
      "Hits: 294.6168108926609\n",
      "AtBat: -206.41191006410185\n",
      "Walks: 87.80892920285068\n",
      "CRBI: 153.23102647057257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'elasticnet__alpha': 0.1, 'elasticnet__l1_ratio': 0.9},\n",
       " np.float64(119175.85350719497))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regression_with_five(Xfive, y, regression_type= \"elasticnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfive = hits[['CRuns', 'Hits', 'AtBat', 'Walks', 'CRBI', 'Division']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def run_regression_with_five(Xfive, y, regression_type):\n",
    "    \"\"\"\n",
    "    Function to run a regression (OLS, Ridge, Lasso, ElasticNet) with cross-validation\n",
    "    and return the best hyperparameter (alpha) and best cross-validated MSE score.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Features DataFrame\n",
    "    y -- Target variable\n",
    "    regression_type -- Type of regression to run: 'ols', 'ridge', 'lasso', 'elasticnet'\n",
    "\n",
    "    Returns:\n",
    "    best_alpha -- Best alpha parameter found via GridSearchCV (or None for OLS)\n",
    "    best_cv_mse -- Best cross-validated MSE score\n",
    "    \"\"\"\n",
    "    # Convert all non-numeric columns to numeric (or handle non-numeric values appropriately)\n",
    "    Xfive = Xfive.apply(pd.to_numeric, errors='coerce')  # Coerce non-numeric to NaN\n",
    "    Xfive.fillna(0, inplace=True)  # Optionally fill NaN values with 0 (or another strategy)\n",
    "\n",
    "    print(\"Preprocessing completed.\")\n",
    "\n",
    "    # Define ColumnTransformer for preprocessing (standardizing numerical, encoding categorical)\n",
    "    numerical_cols = Xfive.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = Xfive.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    ct = ColumnTransformer(\n",
    "        [\n",
    "            (\"standardize\", StandardScaler(), numerical_cols),  # Standard scaling for numerical features\n",
    "            (\"encode\", OneHotEncoder(), categorical_cols),  # One-hot encoding for categorical features\n",
    "        ],\n",
    "        remainder=\"passthrough\"\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    # Add PolynomialFeatures to create interaction terms\n",
    "    interaction = PolynomialFeatures(interaction_only=False, include_bias=False)\n",
    "\n",
    "    # Initialize the regression models\n",
    "    pipeline = None  # Initialize the pipeline variable to avoid UnboundLocalError\n",
    "\n",
    "    if regression_type == 'ols':\n",
    "        print(\"OLS Regression selected.\")\n",
    "        model = LinearRegression()\n",
    "        pipeline = Pipeline([\n",
    "            (\"preprocessing\", ct), \n",
    "            (\"interaction\", interaction),  # Adding interaction terms\n",
    "            (\"regression\", model)\n",
    "        ])\n",
    "        \n",
    "        # Cross-validation MSE score\n",
    "        best_cv_mse = cross_val_score(pipeline, Xfive, y, cv=5, scoring='neg_mean_squared_error').mean() * -1\n",
    "        pipeline.fit(Xfive, y)  # Fit to get the coefficients\n",
    "\n",
    "        # Extract feature names\n",
    "        all_feature_names = list(numerical_cols) + list(categorical_cols)\n",
    "\n",
    "        # Extract coefficients for the fitted model\n",
    "        coefficients = pipeline.named_steps['regression'].coef_\n",
    "\n",
    "        # Display coefficients with feature names\n",
    "        print(\"Feature names and coefficients for OLS (Linear Regression):\")\n",
    "        for feature, coef in zip(all_feature_names, coefficients):\n",
    "            print(f\"{feature}: {coef}\")\n",
    "\n",
    "        return None, best_cv_mse  # No best alpha for OLS\n",
    "\n",
    "    # For Ridge, Lasso, and ElasticNet\n",
    "    elif regression_type in ['ridge', 'lasso', 'elasticnet']:\n",
    "        print(f\"{regression_type} selected.\")\n",
    "        # Set model and hyperparameters based on the regression type\n",
    "        if regression_type == 'ridge':\n",
    "            model = Ridge()\n",
    "            param_grid = {\"ridge__alpha\": [0.001, 0.01, 0.1, 1, 10]}\n",
    "        elif regression_type == 'lasso':\n",
    "            model = Lasso()\n",
    "            param_grid = {\"lasso__alpha\": [0.001, 0.01, 0.1, 1, 10]}\n",
    "        elif regression_type == 'elasticnet':\n",
    "            model = ElasticNet()\n",
    "            param_grid = {\"elasticnet__alpha\": [0.001, 0.01, 0.1, 1, 10],\n",
    "                          \"elasticnet__l1_ratio\": [0.1, 0.5, 0.9]}  # L1 ratio for ElasticNet\n",
    "\n",
    "        # Create pipeline and perform grid search\n",
    "        pipeline = Pipeline([\n",
    "            (\"preprocessing\", ct), \n",
    "            (\"interaction\", interaction),  # Adding interaction terms\n",
    "            (regression_type, model)\n",
    "        ])  # Create pipeline here\n",
    "        \n",
    "        print(\"Performing GridSearchCV...\")\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(Xfive, y)\n",
    "\n",
    "        # Best hyperparameter and cross-validated MSE score\n",
    "        best_alpha = grid_search.best_params_\n",
    "        best_cv_mse = -grid_search.best_score_  # Negating to return positive MSE\n",
    "        \n",
    "        # Display the results\n",
    "        print(f\"Best {regression_type} Alpha: {best_alpha}\")\n",
    "        print(f\"Best Cross-Validated MSE: {best_cv_mse}\")\n",
    "\n",
    "        # Extract the coefficients and feature names for the best model\n",
    "        best_pipeline = grid_search.best_estimator_\n",
    "        coefficients = best_pipeline.named_steps[regression_type].coef_\n",
    "\n",
    "        # Combine feature names (numerical + categorical)\n",
    "        all_feature_names = list(numerical_cols) + list(categorical_cols)\n",
    "\n",
    "        # Display coefficients for the selected model\n",
    "        print(f\"Feature names and coefficients for {regression_type}:\")\n",
    "        for feature, coef in zip(all_feature_names, coefficients):\n",
    "            print(f\"{feature}: {coef}\")\n",
    "\n",
    "        return best_alpha, best_cv_mse\n",
    "\n",
    "    else:\n",
    "        print(f\"Invalid regression type: {regression_type}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "OLS Regression selected.\n",
      "Feature names and coefficients for OLS (Linear Regression):\n",
      "CRuns: -59.499135728742985\n",
      "Hits: 67.9204947447958\n",
      "AtBat: -10.29451192521862\n",
      "Walks: 73.72745757858594\n",
      "CRBI: 400.5530122752484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, np.float64(98527.61453677765))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regression_with_five(Xfive, y, regression_type= 'ols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "ridge selected.\n",
      "Performing GridSearchCV...\n",
      "Best ridge Alpha: {'ridge__alpha': 0.001}\n",
      "Best Cross-Validated MSE: 98530.92869998606\n",
      "Feature names and coefficients for ridge:\n",
      "CRuns: -59.44816133010122\n",
      "Hits: 67.96572596155727\n",
      "AtBat: -10.305761081330058\n",
      "Walks: 73.7043859321674\n",
      "CRBI: 400.4994619937646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'ridge__alpha': 0.001}, np.float64(98530.92869998606))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regression_with_five(Xfive, y, regression_type= 'ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "lasso selected.\n",
      "Performing GridSearchCV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.788e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.151e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.114e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.539e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.627e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.025e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.962e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.005e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.063e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.286e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.966e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.728e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.532e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.272e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.215e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.236e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.575e+05, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.941e+04, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.410e+06, tolerance: 4.558e+03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lasso Alpha: {'lasso__alpha': 0.001}\n",
      "Best Cross-Validated MSE: 96634.71271346684\n",
      "Feature names and coefficients for lasso:\n",
      "CRuns: -42.580735785877145\n",
      "Hits: 69.15167411032984\n",
      "AtBat: -10.185109898279245\n",
      "Walks: 73.62481386687428\n",
      "CRBI: 385.4001126864816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.764e+06, tolerance: 5.332e+03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'lasso__alpha': 0.001}, np.float64(96634.71271346684))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regression_with_five(Xfive, y, regression_type= 'lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "elasticnet selected.\n",
      "Performing GridSearchCV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.090e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.323e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.552e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.823e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.663e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.970e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.252e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.386e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.701e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.468e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.829e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.173e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.177e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.575e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.224e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.588e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.427e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.535e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.964e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.756e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.244e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.115e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.599e+05, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.241e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.873e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.164e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.338e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.680e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.518e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.238e+04, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.836e+05, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.024e+04, tolerance: 4.137e+03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best elasticnet Alpha: {'elasticnet__alpha': 0.001, 'elasticnet__l1_ratio': 0.9}\n",
      "Best Cross-Validated MSE: 97371.00054922434\n",
      "Feature names and coefficients for elasticnet:\n",
      "CRuns: -41.47848558574818\n",
      "Hits: 70.11784955022313\n",
      "AtBat: -10.541538882894747\n",
      "Walks: 73.10759717641653\n",
      "CRBI: 384.25094364882216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.821e+06, tolerance: 5.332e+03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'elasticnet__alpha': 0.001, 'elasticnet__l1_ratio': 0.9},\n",
       " np.float64(97371.00054922434))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_regression_with_five(Xfive, y, regression_type= 'elasticnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lasso model with the 5 variables and their interactions performed the best with an alpha of .001 and an MSE of 96634."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Ridge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This relationship is somewhat inverse since Ridge regression penalizes larger coefficients more than OLS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. LASSO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some were the same, but most were different. This makes sense because different variables being in the model are going to cause different optimal lambdas. The presence of the variables is what is really going to make the difference between variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic net models straddle in between ridge and lasso and can be tuned to be the optimal model between the two approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model was the lasso model with the 5 best variables and their interactions with the categorical variable. The lambda for this model is .001 and the best cross validated MSE is 96634. I chose to show the different MSE's that were used in the cross validation selection as it seems important to me to not only show how different variables can affect a prediction, but also how different lambdas can affect the error of a model too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "lasso selected.\n",
      "Performing GridSearchCV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.788e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.151e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.114e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.539e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.627e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.025e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.962e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.005e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.063e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.286e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.966e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.728e+06, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.532e+06, tolerance: 4.281e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.272e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.215e+06, tolerance: 4.708e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.236e+06, tolerance: 3.606e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.575e+05, tolerance: 4.137e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.941e+04, tolerance: 4.281e+03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lasso Alpha: {'lasso__alpha': 0.001}\n",
      "Best Cross-Validated MSE: 96634.71271346684\n",
      "Feature names and coefficients for lasso:\n",
      "CRuns: -42.580735785877145\n",
      "Hits: 69.15167411032984\n",
      "AtBat: -10.185109898279245\n",
      "Walks: 73.62481386687428\n",
      "CRBI: 385.4001126864816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.410e+06, tolerance: 4.558e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.764e+06, tolerance: 5.332e+03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'lasso__alpha': 0.001}, np.float64(96634.71271346684))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lasso model with 5 variables and their interaction\n",
    "run_regression_with_five(Xfive, y, regression_type= 'lasso') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.247e+06, tolerance: 4.367e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.700e+06, tolerance: 3.931e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.832e+06, tolerance: 3.957e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.907e+06, tolerance: 4.282e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.576e+06, tolerance: 4.765e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.114e+06, tolerance: 4.367e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.577e+06, tolerance: 3.931e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.638e+06, tolerance: 3.957e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.712e+06, tolerance: 4.282e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.425e+06, tolerance: 4.765e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.028e+06, tolerance: 4.367e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.549e+06, tolerance: 3.931e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.084e+06, tolerance: 3.957e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.248e+06, tolerance: 4.282e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.105e+06, tolerance: 4.765e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.385e+05, tolerance: 4.367e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.324e+05, tolerance: 3.931e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.818e+05, tolerance: 3.957e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.339e+05, tolerance: 4.282e+03\n",
      "c:\\Users\\hblin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.787e+05, tolerance: 4.765e+03\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define the preprocessing and model setup\n",
    "def create_pipeline(X):\n",
    "    # Columns: numerical and categorical\n",
    "    numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    # Column transformer (for scaling numerical columns, encoding categorical ones)\n",
    "    ct = ColumnTransformer(\n",
    "        [\n",
    "            (\"standardize\", StandardScaler(), numerical_cols),\n",
    "            (\"encode\", OneHotEncoder(), categorical_cols)\n",
    "        ], remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "    # Add polynomial features (interaction terms)\n",
    "    interaction = PolynomialFeatures(interaction_only=False, include_bias=False)\n",
    "\n",
    "    # Create a pipeline with scaling, encoding, and Lasso regression\n",
    "    lasso = Lasso()\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessing\", ct),\n",
    "        (\"interaction\", interaction),\n",
    "        (\"lasso\", lasso)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Define cross-validation\n",
    "def cross_val_mse(X, y, alpha_values):\n",
    "    mse_scores = []\n",
    "    \n",
    "    # K-Fold Cross-Validation Setup (same folds as GridSearchCV)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)  # Ensure reproducibility\n",
    "    \n",
    "    # Loop over alpha values\n",
    "    for alpha in alpha_values:\n",
    "        model = create_pipeline(X)\n",
    "        model.set_params(lasso__alpha=alpha)  # Set alpha in the Lasso model\n",
    "\n",
    "        # Compute MSE with cross-validation\n",
    "        mse = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "        mse_scores.append(-mse.mean())  # Convert negative MSE to positive\n",
    "        \n",
    "    return mse_scores\n",
    "\n",
    "# Define the alpha values for Lasso regularization\n",
    "alpha_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "# Get cross-validated MSE for different alpha values\n",
    "mse_scores = cross_val_mse(Xfive, y, alpha_values)\n",
    "\n",
    "# Plot MSE vs alpha\n",
    "plt.plot(alpha_values, mse_scores, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('MSE vs Alpha for Lasso Regression')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
